# scan voor robots.txt en sitemap.xml achter website
# https://doc.scrapy.org/en/latest/topics/spiders.html#topics-spiders
# Scrapy Sitemap Spider
# https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/sitemap.html

import pydirbuster

file = "Tools/dirbuster.txt"

def DirbusterScan(website):

    webbuster = pydirbuster.Pybuster(f"https://{website}/", wordfile = file, exts= ['php', 'html', 'js', 'json', 'txt'] )
    data = webbuster.Run()
    return data

def SitemapScan(website):
    pass

def GetRobot(website):
    pass



