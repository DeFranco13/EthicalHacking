# scan voor robots.txt en sitemap.xml achter website
# https://doc.scrapy.org/en/latest/topics/spiders.html#topics-spiders
# Scrapy Sitemap Spider
# https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/sitemap.html

import pydirbuster
import os
import warnings

file = "WebScan/Tools/dirbuster.txt"

def DirbusterScan(website):

    webbuster = pydirbuster.Pybuster(f"https://{website}/", wordfile = file, exts= ['php', 'html', 'js', 'json', 'txt'] )
    result = os.system(f"gobuster {website} {file}")
    data = webbuster.Run()
    return data


def GetRobot(website):
    warnings.filterwarnings("ignore", category=UserWarning, module="os")
    result = os.popen(f"curl https://{website}/robots.txt").read()
    return result



