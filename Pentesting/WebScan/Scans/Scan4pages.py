# scan voor robots.txt en sitemap.xml achter website
# https://doc.scrapy.org/en/latest/topics/spiders.html#topics-spiders
# Scrapy Sitemap Spider
# https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/sitemap.html

import pydirbuster
import os
import warnings

file = "WebScan/Tools/dirbuster.txt"

def DirbusterScan(website):
    pass


def GetRobot(website):
    pass



